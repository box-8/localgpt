# Rag bot :smiley:
De l'intelligence artificielle sur ton ordi !  

ü§ó [huggingface](https://huggingface.co/).


## langue 
La documentation du projet est en fran√ßais, en UTF8.

## objectif 
Minimiser en local le mat√©riel n√©cessaire pour l'emploi des technologies d'IA et en optimiser l'impact üå§Ô∏è.

En V1.0, l'application permet de questionner, discuter avec des propres documents sur un ordinateur non connect√©.
En V1.1, l'application embarque le serveur de mod√®les Llama CPP.


![Interface de gestion Chromadb](images/chromadb.ui.jpg)

# Installation
Pour fonctionner, l'application a besoin de diff√©rents composants.

## python
Langage de programmation de l'application :
- cr√©er un projet python vierge 
- de pr√©f√©rence cr√©er un environnement virtuel (avec venv par exemple)
- installer les librairies du fichier requirements.txt

Les librairies utilis√©es sont : 
- :sparkling_heart: streamlit : pour l'interface utilisateur de l'application (http://localhost:8501 par d√©faut) 
- :point_right: chromadb : pour stocker les documents vectoris√©s ( http://localhost:8000 par d√©faut)
- :stuck_out_tongue: langchain : pour simplifier la cr√©ation de l'applications LLM

## Serveur LLM
L'application ouvre des port appeller les serveurs LLM en http://localhost.  
Deux options : 

### LM Studio (beginner)
- installer LM Studio sur votre ordinateur, t√©l√©charger un LLM (mistral 7b est parfait pour un pc portable), le servir en localhost (port 1234 par d√©faut)  
### llama_cpp[server] (default)
en version 1.1, l'application g√®re la mise en service de plusieurs mod√®les en simultan√©s.
- compiler Llama.cpp

### Embeddings
- par d√©faut l'application utilise le mod√®le d'embeddings hkunlp/instructor-xl disponible sur Huggingface :hugging_face:
- en version 1.1, n'importe quel mod√®le d'embedding peut √™tre utilis√© en cache.

## VectorStore
La base de donn√©es vectorielle impl√©ment√©e est Chromadb.

# Mise en service

## ex√©cuter lmstudio
Lancer LM Studio et servir le LLM 
![Interface de gestion Chromadb](images/lmstudio.ui.jpg)
## llama_cpp[server]
En verion 1.1 l'application int√©gre llama.cpp serveur, comme service de base.

L'interface port√©e avec streamlit permet de lancer autant de mod√®les que n√©cessaire en simultan√©.    

ü§ó [huggingface](https://huggingface.co/).


## ex√©cuter le serveur Chroma
- chroma run --path ./chroma
## ex√©cuter l'application
- streamlit run home.py


# Badges
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[![Derni√®re release](https://img.shields.io/badge/release-first-blue)]({{RELEASE_URL}})


# Visuals
L'application dispose d'une interface graphique avec la base de donn√©es vectorielle chromadb.  

Elle permet : 
- la cr√©ation, la supression de collections de documents 
- l'upload, la visualisation et la suppression de documents sur le serveur et dans les collections chromadb
- la vectorisation des documents dans la base de donn√©es chromadb
- la mise √† jour des propri√©t√©s des documents stock√©s (m√©tadatas)

![Interface de gestion Chromadb](images/chromadb.ui.jpg)

Interroger les documents vectoris√©s dans les collections de la base chromadb.  
Les options permettent : 
- de modifier le contexte de la discussion
- choisir les documents avec lesquels discutter (un ou plusieurs)
- param√©trer le nombre de fragments pertinents √† utiliser
![Interface de gestion Chromadb](images/ragbot.ui.jpg)

Discuter avec le llm  
![Interface de gestion Chromadb](images/chat.ui.jpg)



# More
## Hardware 
- minimum : ryzen 7 32 gb, no GPU  


## Support


## Roadmap
- am√©liorer l'API pour chromadb, gestion modification des metadatas.

- d√©coreller les fonctionnalit√©s m√©tier / prompt / rag de l'interface utilisateur

- encapsulation des prompts m√©tier dans des agents


## Contributing
Toutes contributions sont les bienvenues.

N'h√©sitez pas √† forker le projet v1.1 


## Authors and acknowledgment
Ressources, remerciements, inspirations en ent√™te du code et des commentaires 

## License
Open source

## Project status
üòÑ